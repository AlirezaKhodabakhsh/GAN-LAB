{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Common parts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x25c7f20e210>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "torch.manual_seed(0) # Set for testing purposes, please do not change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, C_noise, C_hidden, C_image):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.gen = nn.Sequential(\n",
    "            self.gen_block(C_noise   , C_hidden*4, K=3, S=2),\n",
    "            self.gen_block(C_hidden*4, C_hidden*2, K=4, S=1),\n",
    "            self.gen_block(C_hidden*2, C_hidden*1, K=3, S=2),\n",
    "            self.gen_block(C_hidden*1  , C_image , K=4, S=2, final_layer=True)\n",
    "        )\n",
    "\n",
    "    def gen_block(self, C_in, C_out, K, S, final_layer=False):\n",
    "        if final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(C_in, C_out, kernel_size = K, stride = S),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(C_in, C_out, kernel_size = K, stride = S),\n",
    "                nn.BatchNorm2d(C_out),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, C_image, C_hidden):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.dis = nn.Sequential(\n",
    "            self.dis_block(C_image   , C_hidden*1, K=4, S=2),\n",
    "            self.dis_block(C_hidden*1, C_hidden*2, K=4, S=2),\n",
    "            self.dis_block(C_hidden*2, 1         , K=4, S=2, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def dis_block(self, C_in, C_out, K, S, final_layer=False):\n",
    "        if final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(C_in, C_out, kernel_size = K, stride = S)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(C_in, C_out, kernel_size = K, stride = S),\n",
    "                nn.BatchNorm2d(C_out),\n",
    "                nn.LeakyReLU(inplace=True, negative_slope=0.2)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dis(x).view(x.shape[0], -1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_noise(N_noise, C_noise, device='cpu'):\n",
    "    return torch.randn(N_noise, C_noise, device=device).view(-1, C_noise, 1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "C_noise = 64\n",
    "display_step = 500\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "c_lambda = 10\n",
    "crit_repeats = 5\n",
    "device = 'cuda'\n",
    "C_hidden_gen = 64\n",
    "C_hidden_dis = 16\n",
    "C_image=1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    MNIST('.', download=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Coursera"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_gradient_1(crit, real, fake, epsilon):\n",
    "    mixed_images = real * epsilon + fake * (1 - epsilon)\n",
    "    mixed_scores = crit(mixed_images)\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=mixed_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    return gradient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def gradient_penalty_1(gradient):\n",
    "    gradient = gradient.view(len(gradient), -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    penalty = ( ( gradient_norm - 1.0 )**2 ).mean(dim=0)\n",
    "    return penalty"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_gen_loss_1(crit_fake_pred):\n",
    "    gen_loss = (-crit_fake_pred).mean(dim=0)\n",
    "    return gen_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_crit_loss_1(crit_fake_pred, crit_real_pred, gp, c_lambda):\n",
    "    crit_loss = ( crit_fake_pred - crit_real_pred + c_lambda*gp ).mean(dim=0)\n",
    "    return crit_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# get instance from models\n",
    "gen = Generator(C_noise, C_hidden_gen, C_image).to(device)\n",
    "dis = Discriminator(C_image, C_hidden_dis).to(device)\n",
    "\n",
    "# Initialize\n",
    "gen = gen.apply(weights_init)\n",
    "dis = dis.apply(weights_init)\n",
    "\n",
    "# Optimizers\n",
    "optim_dis = torch.optim.Adam(dis.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "optim_gen = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([8.1294], device='cuda:0', grad_fn=<MeanBackward1>)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RESULT\n",
    "real = torch.ones(128,1,28,28).to(device)\n",
    "cur_batch_size = len(real)\n",
    "fake_noise = get_noise(cur_batch_size, C_noise, device=device)\n",
    "fake = gen(fake_noise)\n",
    "crit_fake_pred = dis(fake.detach())\n",
    "crit_real_pred = dis(real)\n",
    "epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n",
    "gradient = get_gradient_1(dis, real, fake.detach(), epsilon)\n",
    "gp = gradient_penalty_1(gradient)\n",
    "crit_loss = get_crit_loss_1(crit_fake_pred, crit_real_pred, gp, c_lambda)\n",
    "crit_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/469 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93d9611a65c44bb8af363b9c30943c21"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/469 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06db81f280804ebd86ac2fe436c600fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_8996/1918106188.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     27\u001B[0m             \u001B[0mmean_iteration_critic_loss\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mcrit_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mcrit_repeats\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m             \u001B[1;31m# Update gradients\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m             \u001B[0mcrit_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m             \u001B[1;31m# Update optimizer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m             \u001B[0moptim_dis\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    305\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    306\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 307\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    308\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    309\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mB:\\Anaconda_install_path\\envs\\deeplearning\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    152\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 154\u001B[1;33m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[0;32m    155\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    156\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "critic_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches\n",
    "    for real, _ in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        real = real.to(device)\n",
    "\n",
    "        mean_iteration_critic_loss = 0\n",
    "        for _ in range(crit_repeats):\n",
    "            ### Update critic ###\n",
    "            optim_dis.zero_grad()\n",
    "            fake_noise = get_noise(cur_batch_size, C_noise, device=device)\n",
    "            fake = gen(fake_noise)\n",
    "            crit_fake_pred = dis(fake.detach())\n",
    "            crit_real_pred = dis(real)\n",
    "\n",
    "            epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n",
    "            gradient = get_gradient_1(dis, real, fake.detach(), epsilon)\n",
    "            gp = gradient_penalty_1(gradient)\n",
    "            crit_loss = get_crit_loss_1(crit_fake_pred, crit_real_pred, gp, c_lambda)\n",
    "\n",
    "            # Keep track of the average critic loss in this batch\n",
    "            mean_iteration_critic_loss += crit_loss.item() / crit_repeats\n",
    "            # Update gradients\n",
    "            crit_loss.backward(retain_graph=True)\n",
    "            # Update optimizer\n",
    "            optim_dis.step()\n",
    "        critic_losses += [mean_iteration_critic_loss]\n",
    "\n",
    "        ### Update generator ###\n",
    "        optim_gen.zero_grad()\n",
    "        fake_noise_2 = get_noise(cur_batch_size, C_noise, device=device)\n",
    "        fake_2 = gen(fake_noise_2)\n",
    "        crit_fake_pred = dis(fake_2)\n",
    "\n",
    "        gen_loss = get_gen_loss_1(crit_fake_pred)\n",
    "        gen_loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optim_gen.step()\n",
    "\n",
    "        # Keep track of the average generator loss\n",
    "        generator_losses += [gen_loss.item()]\n",
    "\n",
    "        ### Visualization code ###\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "            crit_mean = sum(critic_losses[-display_step:]) / display_step\n",
    "            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {gen_mean}, critic loss: {crit_mean}\")\n",
    "            show_tensor_images(fake)\n",
    "            show_tensor_images(real)\n",
    "            step_bins = 20\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Generator Loss\"\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(critic_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Critic Loss\"\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        cur_step += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Me"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def get_epsilon(N_epsilon, device='cpu'):\n",
    "    return torch.rand(N_epsilon, 1, 1, 1, device=device, requires_grad=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def get_gradient_2(dis, data):\n",
    "    data.requires_grad_()\n",
    "    score = dis(data)\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=data,\n",
    "        outputs=score,\n",
    "        grad_outputs=torch.ones_like(score),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    return gradient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def gradient_penalty_2(gradient):\n",
    "    gradient = gradient.view(len(gradient), -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    penalty = ( ( gradient_norm - 1.0 )**2 ).mean(dim=0)\n",
    "    return penalty"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def get_loss_dis_2(gen, dis,\n",
    "                real,\n",
    "                N_noise, C_noise,\n",
    "                c_lambda,\n",
    "                device):\n",
    "    fake = gen(get_noise(N_noise, C_noise, device))\n",
    "    epsilon = get_epsilon(N_epsilon = N_noise, device=device)\n",
    "    mixed_images = real * epsilon + fake * (1 - epsilon)\n",
    "    gp = gradient_penalty_2(get_gradient_2(dis, mixed_images))\n",
    "    return ( dis(mixed_images) - dis(real) + c_lambda * gp ).mean(dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def get_loss_gen_2(gen, dis,\n",
    "                N_noise, C_noise,\n",
    "                device):\n",
    "    fake = gen(get_noise(N_noise, C_noise, device))\n",
    "    return torch.mean( -dis(fake) )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# get instance from models\n",
    "gen = Generator(C_noise, C_hidden_gen, C_image).to(device)\n",
    "dis = Discriminator(C_image, C_hidden_dis).to(device)\n",
    "\n",
    "# Initialize\n",
    "gen = gen.apply(weights_init)\n",
    "dis = dis.apply(weights_init)\n",
    "\n",
    "# Optimizers\n",
    "optim_dis = torch.optim.Adam(dis.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "optim_gen = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([7.9603], device='cuda:0', grad_fn=<MeanBackward1>)"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real = torch.ones(128,1,28,28).to(device)\n",
    "cur_batch_size = len(real)\n",
    "loss_dis = get_loss_dis_2(gen, dis, real, cur_batch_size, C_noise, c_lambda, device)\n",
    "loss_dis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/469 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b19357ed8134a1c8d12288f98d9bd94"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_8996/3169357325.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m             \u001B[1;31m# Keep track of the average critic loss in this batch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 23\u001B[1;33m             \u001B[0mmean_iteration_critic_loss\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mcrit_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mcrit_repeats\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     24\u001B[0m             \u001B[1;31m# Update gradients\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m             \u001B[0mcrit_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "critic_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches\n",
    "    for real, _ in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        real = real.to(device)\n",
    "\n",
    "        mean_iteration_critic_loss = 0\n",
    "        for _ in range(crit_repeats):\n",
    "            ### Update critic ###\n",
    "            optim_dis.zero_grad()\n",
    "            crit_loss = get_loss_dis_2(gen, dis,\n",
    "                real,\n",
    "                cur_batch_size, C_noise,\n",
    "                c_lambda,\n",
    "                device)\n",
    "\n",
    "            # Keep track of the average critic loss in this batch\n",
    "            mean_iteration_critic_loss += crit_loss.item() / crit_repeats\n",
    "            # Update gradients\n",
    "            crit_loss.backward(retain_graph=True)\n",
    "            # Update optimizer\n",
    "            optim_dis.step()\n",
    "        critic_losses += [mean_iteration_critic_loss]\n",
    "\n",
    "        ### Update generator ###\n",
    "        optim_gen.zero_grad()\n",
    "\n",
    "        gen_loss = get_loss_gen_2(gen, dis,\n",
    "                cur_batch_size, C_noise,\n",
    "                device)\n",
    "        gen_loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optim_gen.step()\n",
    "\n",
    "        # Keep track of the average generator loss\n",
    "        generator_losses += [gen_loss.item()]\n",
    "\n",
    "        ### Visualization code ###\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "            crit_mean = sum(critic_losses[-display_step:]) / display_step\n",
    "            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {gen_mean}, critic loss: {crit_mean}\")\n",
    "# new\n",
    "            fake_noise = get_noise(cur_batch_size, C_noise, device=device)\n",
    "            fake = gen(fake_noise)\n",
    "# end new\n",
    "            show_tensor_images(fake)\n",
    "            show_tensor_images(real)\n",
    "            step_bins = 20\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Generator Loss\"\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(critic_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Critic Loss\"\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        cur_step += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}